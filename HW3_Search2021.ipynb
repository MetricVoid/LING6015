{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wob59VHd6fIQ"
   },
   "source": [
    "### Intro\n",
    "\n",
    "This assignment is adapted from this blog post (https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089) and Notebook (https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb) by Scott Williams.  All credit goes to him!\n",
    "\n",
    "The instructions here assume that you are using Anaconda on your own computer (that is what I am doing myself.) In class, maybe we can all take a look at this and help each other get it up and running also on Colab.\n",
    "\n",
    "This homework might be confusing, but you don't have to WRITE any code; you just have to understand it. Even if you aren't a proficient coder, you probably have a lot of (humanistic!!) skills in reading and understanding things that are somewhat confusing, so just do your best and learn as much as you can.  It is okay to be confused -- that is PART OF LEARNING!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCNyYElM6fIV"
   },
   "source": [
    "First download the data here: http://archives.textfiles.com/stories.zip.  I downloaded it and unzipped it on my Desktop which is also the same location as this Notebook, personally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeI8aqN_6fIW"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7349,
     "status": "ok",
     "timestamp": 1631214182844,
     "user": {
      "displayName": "Lelia Glass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggto8ET9u52j0dToj9z_8G1aIgATHxCDkbMe5epTQ=s64",
      "userId": "02421567402523508095"
     },
     "user_tz": 240
    },
    "id": "gvVJQ_E86fIW",
    "outputId": "0cd5668b-86b5-4168-c8ea-cb1331483ad2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in /home/maki/miniconda3/envs/ling/lib/python3.7/site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/maki/miniconda3/envs/ling/lib/python3.7/site-packages (from num2words) (0.6.2)\n",
      "--2021-09-15 23:05:00--  http://archives.textfiles.com/stories.zip\n",
      "Resolving archives.textfiles.com (archives.textfiles.com)... 208.86.224.90\n",
      "Connecting to archives.textfiles.com (archives.textfiles.com)|208.86.224.90|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5227911 (5.0M) [application/zip]\n",
      "Saving to: ‘stories.zip.1’\n",
      "\n",
      "stories.zip.1       100%[===================>]   4.99M  1.85MB/s    in 2.7s    \n",
      "\n",
      "2021-09-15 23:05:03 (1.85 MB/s) - ‘stories.zip.1’ saved [5227911/5227911]\n",
      "\n",
      "Archive:  stories.zip\n",
      "replace stories/100west.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "!pip install num2words\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "!wget http://archives.textfiles.com/stories.zip\n",
    "!unzip stories.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1631214193196,
     "user": {
      "displayName": "Lelia Glass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggto8ET9u52j0dToj9z_8G1aIgATHxCDkbMe5epTQ=s64",
      "userId": "02421567402523508095"
     },
     "user_tz": 240
    },
    "id": "YuTQpxG36fIY"
   },
   "outputs": [],
   "source": [
    "title = \"stories\"\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1631214194702,
     "user": {
      "displayName": "Lelia Glass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggto8ET9u52j0dToj9z_8G1aIgATHxCDkbMe5epTQ=s64",
      "userId": "02421567402523508095"
     },
     "user_tz": 240
    },
    "id": "rkNRljL26fIZ"
   },
   "outputs": [],
   "source": [
    "folders = [x[0] for x in os.walk(str(os.getcwd())+'/'+title+'/')]\n",
    "folders[0] = folders[0][:len(folders[0])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1631214221968,
     "user": {
      "displayName": "Lelia Glass",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggto8ET9u52j0dToj9z_8G1aIgATHxCDkbMe5epTQ=s64",
      "userId": "02421567402523508095"
     },
     "user_tz": 240
    },
    "id": "kGfXixUJ6fIZ",
    "outputId": "cb1333ed-f308-4b57-8c22-15bf9b8d9e2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/maki/Documents/LING6015/stories',\n",
       " '/home/maki/Documents/LING6015/stories/SRE',\n",
       " '/home/maki/Documents/LING6015/stories/FARNON']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pjBvggK6fIa"
   },
   "source": [
    "### Question 0\n",
    "\n",
    "Please make sure you can open \"stories\".  We will try to do this in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny9_YVq16fIa"
   },
   "source": [
    "### Your answer to Question 0\n",
    "\n",
    "Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RYMLd0z6fIb"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Please read the Computer Assited Language Learning chapter of our \"textbook\" and use bullet points to indicate 3 things you learned and/or constructive comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0y5-pXN6fIb"
   },
   "source": [
    "### Your answer to Question 1\n",
    "\n",
    "##### What I learned\n",
    "- L1 could facilitate and hinder the learning of L2. Cognates happen when L1 and L2 has similar word with similar meanings, such as coffee in French and Mandarin. This facilitates L2 learning. But over-regulization happens with over-extension of word meanings, such as \"car\" as to \"truck\" or \"van\".\n",
    "- A/B testing describes when 2 groups are given slightly different conditions and researchers compared the outcome variables. This is usually used to test new features. \n",
    "\n",
    "##### Advice/ Comments \n",
    "- The case study of Duolingo is a bit messy. Would be better if there were examples of concepts mentioned in previous text in Duolingo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4xWjVKJ6fIc"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Please read the Search chapter of our textbook and use bullet points to indicate 3 things you learned and/or constructive comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1cip-C36fIc"
   },
   "source": [
    "### Your answer to Question 2\n",
    "\n",
    "#### What I learned\n",
    "- precision is the ratio of relevant results and total results returned. Recall is the ratio of relevant results returned and relevant results out there on the internet. It is impossible to precisely calculate recall since we don't know all the information on the web.\n",
    "- Structured data provides relevant information and help users to better gather all the information about a certain topic. IMDb is such an example. It also gives a knowledge graph of the information about the entity and relations between entities.\n",
    "\n",
    "#### Comments\n",
    "- The boolean search chapter is easy to comprehend after listening to the lecture. Yet from reading books only, the information is a bit too dense. Would be better if there're sub titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJeRPe5x6fIc"
   },
   "source": [
    "Now we resume our exercises...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7UUFElZ6fIc"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "What is in the dataset \"stories\"?  What are the stories, how many are there, and where did they come from? It is always good to look at your data before you do anything to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHLcBbas6fId"
   },
   "source": [
    "### Your answer to Question 3\n",
    "\n",
    "Each file in dataset $\\textit{stories}$ is a fiction with a title, author, and content. There are 452 story files altogether, each containing a story, or a chapter from a long story. This dataset comes from http://archives.textfiles.com/stories.zip Content-wise, it looks like the stories come from some online writing forums/ websites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UGPdRzr06fId",
    "outputId": "4b1063c4-7eae-4fd0-ebfd-4ade54d6ee3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 452\n",
      "15 15\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "c = False\n",
    "\n",
    "for i in folders:\n",
    "    file = open(i+\"/index.html\", 'r')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "\n",
    "    if c == False:\n",
    "        file_name = file_name[2:]\n",
    "        c = True\n",
    "        \n",
    "    print(len(file_name), len(file_title))\n",
    "\n",
    "    for j in range(len(file_name)):\n",
    "        dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YcNeR7dv6fId",
    "outputId": "8ff35a2d-6c44-4e9b-f577-c90874fd3248"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YIZW2DAS6fId"
   },
   "outputs": [],
   "source": [
    "N = len (dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MUmY3-gz6fIe"
   },
   "outputs": [],
   "source": [
    "# given a doc id and a number n, prints out the first n characters from that doc.\n",
    "\n",
    "def print_doc(id,n):\n",
    "    print(dataset[id])\n",
    "    file = open(dataset[id][0], 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    print(text[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "91YmE1-k6fIe",
    "outputId": "5965d8f2-9c57-4bf6-e3a4-a505ff171e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/home/maki/Documents/LING6015/stories/bigred.hum', 'Big Red Riding Cape')\n",
      "BIG RED RIDING CAPE\n",
      "\t\t\t\tAUTHOR UNKNOWN\n",
      "\n",
      "\n",
      "   Once upon a time there was a little girl who had a red cape and a red hood.\n",
      "She was on her way to grandma's house with a basket of food because grandna was\n",
      "hungry and tooolazy to feed herself.  The girl had to go through the woods to\n",
      "get to grandma's house because thats the kind of story this is.  Onthe way she\n",
      "sang a song she wrote.\tHere are the words.\n",
      "\n",
      "\n",
      "     A tisket, a tasket,\n",
      "     A green and yellow basket.\n",
      "     I put a letter in the top And on the way I dropped it.\n",
      "     I dropped it, I dropped it\n",
      "     And on the way I dropped it.\n",
      "     A little boy picked it up\n",
      "     And put it in his pocket.\n",
      "\n",
      "\n",
      "   Yeah, the chick really cooked.  But it was this very song that got her into\n",
      "trouble.  The wolf heard her from a long way off and decided that he was hugnry.\n",
      "He figured he had three options:  he could steal the basket and eat the food; he\n",
      "could steal Little Red Riding Hoood and eat her; or he could steal the song and\n",
      "make a bundle and never be hun\n"
     ]
    }
   ],
   "source": [
    "print_doc(57, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1QYoqt06fIe"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q6FHmDCj6fIe"
   },
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qih_aTx-6fIf",
    "outputId": "72f0c2e9-2458-4072-f4e2-01479aea9a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULdV23Qy6fIf"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "What are stopwords?  (Feel free to use the web).  Here, we are eventually trying to find the most relevant document to a query by looking at the words that appear in both the query and the doc.  Why would it make sense to remove stopwords for that purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y6yWeUs6fIf"
   },
   "source": [
    "### Your answer to Question 4\n",
    "\n",
    "- Stop words are a set of words commonly used in a language yet convey very little useful information. \n",
    "- We are removing stopwords because those words appear in almost every text and don't really show the correlation in meanings. For example, two articles both using a lot of \"is\" may have nothing in common as for content. In order to get a better TF-IDF resultwith higher computation and space efficiency, we should remove those stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lwXxwBZx6fIf"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Be6lrn9G6fIf"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IvOUXiJa6fIg"
   },
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dmCjjbdJ6fIg"
   },
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LSNCGIbW6fIg"
   },
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TIOgJFym6fIg"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5zTnlSY6fIg"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "What just happened here, in a big-picture sense? What did preprocess(data) do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N0u_HN36fIg"
   },
   "source": [
    "### Your answer to Question 5\n",
    "\n",
    "- What happened is we preprocess the data so that the inflected forms of the same word can be recognized as the same word by our program.\n",
    "- Specifically, in the preprocessing, we first convert every word to lower case, then remove punctation and apostrophe, then remove stop words and convert numbers, then stemming and repeated several operations before. Notice the reason $\\textit{stemming(data)},\\textit{remove_punctuation(data)},\\textit{convert_numbers(data)},\\textit{remove_stop_words(data)}$ happened multiple times is because some steps cause not-stemmed words, punctations and stop words to appear, and we need to clean the data deeper. \n",
    "- One example is \"101\" to \"one hundred and one\". Notice there's an \"and\" appearing, which belongs to stopwords. The order of functions help to fix this situation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tkCauYD-6fIh"
   },
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "processed_title = []\n",
    "\n",
    "for i in dataset[:N]:\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIqXkjFH6fIh"
   },
   "source": [
    "### Calculate DF (Document Freq) for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kZT4rUs16fIh"
   },
   "outputs": [],
   "source": [
    "DF = {} # this is a dictionary keeping track of the TOTAL NUMBER OF DOCUMENTS in which a given word appears.\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "XIkwhDJZ6fIh",
    "outputId": "cd6ece6e-0f31-4f65-cff4-8e9f99f24981"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF[\"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0akEWdyu6fIh"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "What does DF[\"flower\"] = 82 mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kurMoglK6fIi"
   },
   "source": [
    "### Your answer to Question 6\n",
    "\n",
    "In $\\textit{stories}$ dataset, there are 82 documents in which the word \"flower\" appears. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EW2x3g266fIi"
   },
   "outputs": [],
   "source": [
    "total_vocab_size = len(DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xLJZD4826fIi",
    "outputId": "519a6e29-f999-42d3-867e-87507b4e274c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32350"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cuBwBX7t6fIi"
   },
   "outputs": [],
   "source": [
    "total_vocab = [x for x in DF]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "sfMFex-_6fIj",
    "outputId": "d6c5ce26-70a6-400b-9ada-1b0b0eaabd0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sharewar', 'trial', 'project', 'freewar', 'need', 'support', 'continu', 'one', 'hundr', 'west', 'fifti', 'three', 'north', 'jim', 'prentic', 'copyright', 'thousand', 'nine', 'nineti', 'brandon']\n"
     ]
    }
   ],
   "source": [
    "print(total_vocab[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YAbwsUQ06fIj"
   },
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "c6AWufpz6fIj",
    "outputId": "7a9fcb4d-5ae1-406c-bc04-9996673c38aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freq(\"flower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ahx7cTOS6fIj"
   },
   "source": [
    "## Calculating TF-IDF for body, we will consider this as the actual tf-idf as we will add the title weight to this.¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dtJ30Fsc6fIk"
   },
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hwZjpDkS6fIk",
    "outputId": "32f17ca9-5d57-483e-db88-8aa54b861fa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021720374140317203"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[200, \"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "rnLVRg8c6fIk",
    "outputId": "30ee30f4-94c0-46f1-ef96-90acbddb4f67"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(120, 'flower')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_133046/3406369604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flower\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (120, 'flower')"
     ]
    }
   ],
   "source": [
    "tf_idf[120, \"flower\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY5kjlmB6fIl"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Why do we get an error when we try to get tf_idf of \"flower\" in Doc 120?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer to Question 7\n",
    "- We get an error running $\\texttt{tf_idf[120,\"flower\"]}$ because the preprocessed doc doesn't contain the word \"flower\". Therefore there's no \"counter[token]\" value returned, which causes the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ijMUaoSe6fIl",
    "outputId": "551ca3bb-2cf3-45af-f750-0ec6f93082a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002906893990853149"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[(0,\"go\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4Y5mj6iI6fIl",
    "outputId": "c4e0a51d-ca1a-40e0-8a17-cb638bee3e6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344095"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLly2Aln6fIm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gnz5g2lO6fIm"
   },
   "source": [
    "### TF-IDF Matching Score Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "uGlw-8Dx6fIm",
    "outputId": "f2543454-659f-475e-faa7-7af268ee676c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\n",
      "\n",
      "['without', 'drive', 'rebeccah', 'insist', 'kate', 'lost', 'momentum', 'stood', 'next', 'slat', 'oak', 'bench', 'canist', 'still', 'clutch', 'survey']\n",
      "\n",
      "[200, 166, 352, 433, 211, 350, 175, 187, 188, 294]\n"
     ]
    }
   ],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    print(l)\n",
    "    \n",
    "\n",
    "matching_score(10, \"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E--HKBzz6fIo"
   },
   "source": [
    "### TF-IDF Cosine Similarity Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "XKhSh9WV6fIo"
   },
   "outputs": [],
   "source": [
    "\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUGI4TJ26fIo"
   },
   "source": [
    "Cosine similarity is a measure of how similar two vectors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "633rdpco6fIo",
    "outputId": "495eebdf-021b-4d51-a8ed-feec12548e4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5773502691896258"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim([1,1,1], [0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCzYBFl86fIp",
    "outputId": "b1b095d7-7735-477a-8c91-0443dbcaddbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0000000000000002"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim([1,1,1], [-1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1mLoiSl6fIp",
    "outputId": "c7a696cd-f95a-4372-938f-b8beafa161ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9914601339836675"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim([1,2,3], [1,2,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3mD9LLT6fIp"
   },
   "source": [
    "### Vectorising tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "RjcNDnto6fIp"
   },
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "BKWBXB4_6fIp"
   },
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    # makes a vector of all 0s, with the same length as the number of unique words in the vocab.\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        # for each word in \"tokens\"\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "            print(tf*idf)\n",
    "            ### ... change its value in our vector of 0s to its tfidf.\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "UicB7Fha6fIp",
    "outputId": "53f2382f-4c0f-40f0-9648-d934d3cec0e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7296276881210488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_vector([\"flower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HmM83GJ6fIq"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "The vector for \"flower\" is a vector of all 0s except for the position corresponding to the word \"flower\", which is 1.73.  What does 1.73 mean?  Why are the rest of the values 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kToTb9Bc6fIq"
   },
   "source": [
    "### Your answer to Question 8\n",
    "\n",
    "- 1.73 means the $\\texttt{tf-idf}$ value of \"flower\" is 1.73. It was calculated by $\\texttt{tf * log((N+1)/(df+1))}$. \n",
    "- The rest of the values are 0 because the token \"flower\" only has one word. $\\texttt{np.unique(tokens)}$ loops once with token value \"flower\". Therefore it only prints tf-idf for \"flower\", which is 1.73. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "KvNxdq2e6fIq",
    "outputId": "0b127cd0-a5b6-42bc-e7b3-1b94298ed407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "\n",
      "Query: rabbit fox\n",
      "\n",
      "['rabbit', 'fox']\n",
      "1.464796235524723\n",
      "1.4081318928712214\n",
      "\n",
      "[  1 187 188 332 186  27 405  26 267 286]\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(k, query): \n",
    "    # k is the number of documents we want to return.\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(out)\n",
    "\n",
    "#     for i in out:\n",
    "#         print(i, dataset[i][0])\n",
    "\n",
    "Q = cosine_similarity(10, \"rabbit fox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cmge06i6fIq"
   },
   "source": [
    "Doc 1 is the \"most similar\" to the query \"rabbit fox\", then Doc 187, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On-FZhse6fIq"
   },
   "source": [
    "Take a look at the first 1000 characters of Doc 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55j8ynSz6fIq",
    "outputId": "7ac6093a-c247-47b3-86da-4ce3a548bfe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Users/lglass9/Desktop/stories/13chil.txt', 'The Story of the Sly Fox')\n",
      "FOR CHILDREN:\n",
      "\n",
      "                                   Sly Fox\n",
      "\n",
      "    Mr. Rabbit sat on his front porch rocking, eating a great big carrot, \n",
      "and looking.\n",
      "\n",
      "    \"Looks like Sly Fox coming down the road,\" he said to himself, walking \n",
      "to the end of the porch. Shading his eyes with his paws, he exclaimed, \"It \n",
      "is Sly Fox.\" \n",
      "\n",
      "    \"Good morning Mr. Rabbit,\" cried Sly Fox, as he walked across the yard. \n",
      "\"Good morning,\" replied Mr. Rabbit, a slight frown on his face. \n",
      "\n",
      "    \"Well,\" said Sly Fox, \"as I haven't seen you in so long a time, thought \n",
      "I would stop and chat a while.\" \n",
      "\n",
      "    Mr. Rabbit could not be rude in his own home, even to an enemy, so he \n",
      "offered Sly Fox a seat on the porch. \n",
      "\n",
      "    \"Take a chair,\" he said politely. \tBut Sly Fox did not stay long, and as \n",
      "he was leaving, he asked: \"Mr. Rabbit, my mother is having a good dinner \n",
      "tonight. Won't you, Mrs. Rabbit, and your three little rabs come to dinner \n",
      "with me?\" \n",
      "\n",
      "    Oh, thought Mr. Rabbit, he knows about my little rabs and wants to take \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_doc(1, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS2J-qeL6fIr"
   },
   "source": [
    "Take a look at the first 1000 characters of Doc 166."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awlWC9Zq6fIr",
    "outputId": "dc61223e-6125-455d-ad30-aa8ff6e2d585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Users/lglass9/Desktop/stories/foxngrap.txt', 'The Story of the Fox and the Grapes')\n",
      "FOX AND THE GRAPES\n",
      "\n",
      "   Once upon a time . . . in a wood there lived a very crafty quick-witted \n",
      "fox. The rabbits, rats, the birds and all the other creatures fled at the \n",
      "sight of him, for they all knew how cruel and famished he was. And since his \n",
      "prey kept fearfully out of sight, the fox had no choice but to haunt the \n",
      "neighbourhood buildings in the hope of finding something to eat. The first \n",
      "time, he was in luck. Near a lonely peasant's cottage, only a low fence stood \n",
      "between him and the hen run, and there he left death and destruction behind \n",
      "him.\n",
      "   \"What careless men, leaving such tender fat hens unguarded,\" he said to \n",
      "himself as he trotted away, still munching.\n",
      "   A few days later, hungry once more, he decided to visit the same hen run \n",
      "again. He crept up to the fence. A thread of smoke curled from the cottage \n",
      "chimney, but all was quiet. With a great bound, he leapt into the hen run. The\n",
      "cackling hens scattered, and the fox was already clutching one in his jaws \n",
      "when a stone\n"
     ]
    }
   ],
   "source": [
    "print_doc(187, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPyXu40I6fIr",
    "outputId": "f1004dcf-051d-4041-8353-02de8115851a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "\n",
      "Query: flower\n",
      "\n",
      "['flower']\n",
      "1.7296276881210488\n",
      "\n",
      "[200  89  84 279  93 130 433 418 189 261]\n"
     ]
    }
   ],
   "source": [
    "Q = cosine_similarity(10, \"flower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzPtqR4I6fIr",
    "outputId": "c65567bd-7e88-45db-98f0-42e84d2b9b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Users/lglass9/Desktop/stories/ghost', 'Time for Flowers, by Gay Bost')\n",
      "TIME FOR FLOWERS\n",
      "  by Gay Bost\n",
      "\n",
      "They'd put flowers up. She hadn't noticed. Time wouldn't hold still.\n",
      "She remembered, quite clearly, that time had been a simple thing; one\n",
      "moment following the previous one, seconds strung out neatly like her\n",
      "mother's pearls laid out on the dark mahogany vanity each Sunday\n",
      "morning. But there had been a catch . . . \n",
      "\n",
      "Hung around Mother's neck the catch clicked and the tidy little line \n",
      "of seconds became a never ending circle with only the catch in the \n",
      "middle. For some reason the thought of pearls gathered from the sea, \n",
      "naturally nested within the confines of oyster shells, scattered \n",
      "haphazardly about the ocean floor disturbed her.\n",
      "\n",
      "Now they'd put up the flowers in the same careless groupings. This,\n",
      "too, disturbed her. Bright yellow trumpets, their collars spread to\n",
      "catch the sun, dotted the front yard in clusters of two or three, five\n",
      "or six. Bunches laid carelessly and forgotten. In a moment she'd\n",
      "come away from the window and have a word with the gar\n"
     ]
    }
   ],
   "source": [
    "print_doc(200, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8Pzr-bs6fIr",
    "outputId": "5c6ababd-c7ed-43e5-d8a4-1100748cf6a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Users/lglass9/Desktop/stories/bulnoopt.txt', 'No Options Left, by G. Daniel Flower')\n",
      "No Option Left\n",
      "\n",
      "           Copyright 1987 by G. Daniel Flower, All Rights Reserved. \n",
      "                            Comments on this\n",
      "     story may be directed to Sparks in the Email or General message\n",
      "             section, and all such comments are invited.\n",
      "\n",
      "\n",
      "\n",
      "                               No Option Left\n",
      "                            by G. Daniel Flower\n",
      "\n",
      "          Before you say anything, I know what I did was wrong. I\n",
      "     freely admit that. Can I but wish that it had never happened.\n",
      "     Unfortunately, the hands of time continue to move forward,\n",
      "     turning back for no man.\n",
      "          I am not a criminal, you know that. Sure, there were the\n",
      "     few shoplifting incidents when I was younger, but Susan had seen\n",
      "     me one day. Needless to say, you were hardly amused by the\n",
      "     situation.\n",
      "          Other than that I only have a few traffic tickets on my\n",
      "     record. Up until now I have been proud of that fact.\n",
      "          It is amazing what a man will do when his back is against\n",
      "     the w\n"
     ]
    }
   ],
   "source": [
    "print_doc(89, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1LZoZw_6fIr"
   },
   "source": [
    "### Question 9\n",
    "\n",
    "What has just happened here?  How does this simple \"search\" function work?  How does it use tfidf?  How does it use cosine similarity?  How does it represent the query and the documents as vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WpqAhW-6fIr"
   },
   "source": [
    "### Your answer to Question 9\n",
    "\n",
    "#### What happend\n",
    "- What happened when calling $\\texttt{Q = cosine_similarity(10, \"rabbit fox\")}$ is it returns index of the top 10 documents with the highest cosine similarity with the query \"rabbit fox\". \n",
    "- Similarly, when calling $\\texttt{cosine_similarity(10, \"flower\")}$, it returns top 10 documents most similar to the query \"flower\". \n",
    "\n",
    "#### How simple search works\n",
    "- Simple search works by using the cosine-similarity between query and each document in the dataset. In the cosine-similarity function, it calculate the cosine similarity of the query and each document in dataset, then sort by decreasing order based on the score of the query and document. \n",
    "\n",
    "#### How it uses tf-idf & How vectorization works for query and document\n",
    "- For the documents, it represents the vectors in an $\\texttt{N-by-total_vocal_size}$ table D. The value was retrieved from tf_idf table of all [doc,token] pairs which records the tf-idf score after assigning body and title weight. $\\texttt{D[doc_number][vocab_index] }$ has the tf-idf value of that word in corresponding document. \n",
    "- For the tokens in the query, it goes to $\\texttt{gen_vector()}$ and returns a vector of tf-idf value for each word in the tokens. This function calculates tf-idf score based on doc_freq data for each word (token). \n",
    "\n",
    "#### How it uses cosine similarity \n",
    "- cos_sim(a,b) is a helper function which returns the cosine similarity between vector a and b. \n",
    "- cosine_similarity(k,query) function calculates cosine-similarity of each document and the query-vector. And the sort the array to get the top k results to return. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djGwZYOC6fIr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW3_Search2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
