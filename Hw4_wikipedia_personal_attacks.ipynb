{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcXO5vB5_KoE"
   },
   "source": [
    "## Wikipedia Personal Attacks\n",
    "\n",
    "The data and most of the code in this Notebook are taken from Ellery Wulczyn, Nithum Thain, and Lucas Dixon. (paper here: https://arxiv.org/abs/1610.08914, notebook here: https://github.com/ewulczyn/wiki-detox/blob/master/src/figshare/Wikipedia%20Talk%20Data%20-%20Getting%20Started.ipynb)\n",
    "\n",
    "These authors' data contain:\n",
    "\n",
    "- a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "- a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "- a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "\n",
    "Please note that some of these comments contain offensive language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdb0h5SL_KoF"
   },
   "source": [
    "## Building a classifier for personal attacks (code from Wulczyn et al)\n",
    "\n",
    "First we import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zRaKhoy_KoF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LtbFQoh_KoM"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "What are these packages and why are we using them?  (Feel free to Google around.)  It is okay if you do not understand all of this, just do your best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xQdX9QX_KoN"
   },
   "source": [
    "### Your answer to Question 1\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7O4GCdS_KoN"
   },
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQrNoRDj_KoR"
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M43hpsL6_KoV",
    "outputId": "8fd123ba-b2a0-4aa3-b326-1e213b20872d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     comment  year  logged_in  \\\n",
      "rev_id                                                                          \n",
      "37675      `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
      "44816      `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002      False   \n",
      "49851      NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002      False   \n",
      "89320       Next, maybe you could work on being less cond...  2002       True   \n",
      "93890                   This page will need disambiguation.   2002       True   \n",
      "102817     NEWLINE_TOKEN-NEWLINE_TOKENNEWLINE_TOKENImport...  2002       True   \n",
      "103624     I removed the following:NEWLINE_TOKENNEWLINE_T...  2002       True   \n",
      "111032     `:If you ever claimed in a Judaic studies prog...  2002       True   \n",
      "120283     NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENMy apol...  2002       True   \n",
      "128532     `Someone wrote:NEWLINE_TOKENMore recognizable,...  2002       True   \n",
      "133562     NEWLINE_TOKENNEWLINE_TOKEN:Correct. Full biogr...  2002       True   \n",
      "138117     `NEWLINE_TOKENNEWLINE_TOKENCare should be take...  2002       True   \n",
      "155243     NEWLINE_TOKENNEWLINE_TOKEN:If I may butt in  I...  2002       True   \n",
      "177310     NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENOn my  ...  2002       True   \n",
      "192579     `NEWLINE_TOKENNEWLINE_TOKEN:<>>NEWLINE_TOKENNE...  2002       True   \n",
      "201190           gets far more tendentious yet.NEWLINE_TOKEN  2002       True   \n",
      "208009     `NEWLINE_TOKENNEWLINE_TOKENAs a person who has...  2002       True   \n",
      "249432     It's great that we've found a new source of fr...  2001       True   \n",
      "252031     NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2001       True   \n",
      "268558     NEWLINE_TOKENNEWLINE_TOKENI'd like the concept...  2001       True   \n",
      "276906     NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2001       True   \n",
      "286174     NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2002       True   \n",
      "290598     `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLIN...  2001       True   \n",
      "294124     NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2002       True   \n",
      "297866     NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE...  2001       True   \n",
      "317177      See? I was right! ;-)  NEWLINE_TOKENNEWLINE_T...  2002       True   \n",
      "336654     NEWLINE_TOKENNEWLINE_TOKEN:I have checked the ...  2002       True   \n",
      "344567     `NEWLINE_TOKENChanged Macedonia link to Macedo...  2002       True   \n",
      "356383     ` NEWLINE_TOKENNEWLINE_TOKEN:Incidentally, re ...  2002       True   \n",
      "358984     `I removed ``from scratch``. In addition to yo...  2002       True   \n",
      "...                                                      ...   ...        ...   \n",
      "699646005   Don't keep changing my page I made please. Yo...  2016       True   \n",
      "699659494  im soory since when is google images not allow...  2016       True   \n",
      "699660419  what ever you fuggin fagNEWLINE_TOKENQuestion ...  2016       True   \n",
      "699661020  NEWLINE_TOKENNEWLINE_TOKEN== Nice try but no c...  2016       True   \n",
      "699661834  `NEWLINE_TOKENNEWLINE_TOKEN== kys ==NEWLINE_TO...  2016       True   \n",
      "699663770  NEWLINE_TOKENNEWLINE_TOKEN== hi Drmies ==NEWLI...  2016       True   \n",
      "699664687   shut up mind your own business and go fuck so...  2016       True   \n",
      "699667660  This talk page is actually a better place to d...  2016       True   \n",
      "699683891  NEWLINE_TOKENNEWLINE_TOKEN== defunct?==NEWLINE...  2016       True   \n",
      "699698850  NEWLINE_TOKENNEWLINE_TOKENYeah, I realized I c...  2016       True   \n",
      "699702006  NEWLINE_TOKEN:There's some weaseling and pov p...  2016       True   \n",
      "699703322  `NEWLINE_TOKEN:::Yeah and in the earlier sente...  2016       True   \n",
      "699715740  `NEWLINE_TOKEN:::::::::::::Again, WP:NOTAFORUM...  2016       True   \n",
      "699728036                      `  [``Those Were the Days``]`  2016       True   \n",
      "699730832  NEWLINE_TOKENNEWLINE_TOKEN== Japanese Scene ==...  2016      False   \n",
      "699732149  NEWLINE_TOKENI am sorry I was only apologizing...  2016       True   \n",
      "699741197  `NEWLINE_TOKENNEWLINE_TOKEN== Jim1138 ==NEWLIN...  2016       True   \n",
      "699753082  NEWLINE_TOKENNEWLINE_TOKEN== Why oh why... ==N...  2016       True   \n",
      "699755057  NEWLINE_TOKENNEWLINE_TOKEN== Daily Beast Artic...  2016       True   \n",
      "699756053  `The lead also lacks proper citation and sourc...  2016       True   \n",
      "699756185  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThe le...  2016       True   \n",
      "699780538          NEWLINE_TOKEN:::::: Well done, thanks!     2016      False   \n",
      "699813325  `NEWLINE_TOKEN::I'm talking about you making u...  2016       True   \n",
      "699820699  `NEWLINE_TOKENYes, from the word ``Guci`` or `...  2016       True   \n",
      "699822249  `NEWLINE_TOKENNEWLINE_TOKEN:``Comment````. Gen...  2016       True   \n",
      "699848324  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThese ...  2016       True   \n",
      "699851288  NEWLINE_TOKENNEWLINE_TOKENThe Institute for Hi...  2016       True   \n",
      "699857133  NEWLINE_TOKEN:The way you're trying to describ...  2016       True   \n",
      "699891012  NEWLINE_TOKENNEWLINE_TOKEN== Warning ==NEWLINE...  2016       True   \n",
      "699897151  Alternate option===NEWLINE_TOKENIs there perha...  2016       True   \n",
      "\n",
      "                ns   sample  split  \n",
      "rev_id                              \n",
      "37675      article   random  train  \n",
      "44816      article   random  train  \n",
      "49851      article   random  train  \n",
      "89320      article   random    dev  \n",
      "93890      article   random  train  \n",
      "102817        user   random  train  \n",
      "103624     article   random  train  \n",
      "111032     article   random    dev  \n",
      "120283     article   random    dev  \n",
      "128532     article   random  train  \n",
      "133562     article   random  train  \n",
      "138117     article   random  train  \n",
      "155243        user   random   test  \n",
      "177310        user   random   test  \n",
      "192579     article   random  train  \n",
      "201190     article   random    dev  \n",
      "208009        user   random  train  \n",
      "249432     article   random  train  \n",
      "252031     article   random  train  \n",
      "268558     article   random  train  \n",
      "276906     article   random  train  \n",
      "286174     article   random   test  \n",
      "290598     article   random    dev  \n",
      "294124     article   random  train  \n",
      "297866     article   random  train  \n",
      "317177     article   random   test  \n",
      "336654        user   random    dev  \n",
      "344567     article   random  train  \n",
      "356383     article   random  train  \n",
      "358984     article   random    dev  \n",
      "...            ...      ...    ...  \n",
      "699646005     user  blocked  train  \n",
      "699659494     user  blocked    dev  \n",
      "699660419     user  blocked   test  \n",
      "699661020     user  blocked  train  \n",
      "699661834     user  blocked   test  \n",
      "699663770     user  blocked  train  \n",
      "699664687     user  blocked    dev  \n",
      "699667660  article  blocked  train  \n",
      "699683891  article  blocked  train  \n",
      "699698850     user  blocked   test  \n",
      "699702006  article  blocked  train  \n",
      "699703322  article  blocked   test  \n",
      "699715740  article  blocked    dev  \n",
      "699728036  article   random  train  \n",
      "699730832  article  blocked    dev  \n",
      "699732149     user   random  train  \n",
      "699741197     user  blocked    dev  \n",
      "699753082     user  blocked   test  \n",
      "699755057  article  blocked    dev  \n",
      "699756053  article  blocked    dev  \n",
      "699756185  article  blocked  train  \n",
      "699780538     user  blocked    dev  \n",
      "699813325  article  blocked  train  \n",
      "699820699     user  blocked    dev  \n",
      "699822249  article  blocked    dev  \n",
      "699848324  article  blocked  train  \n",
      "699851288  article  blocked   test  \n",
      "699857133  article  blocked  train  \n",
      "699891012     user  blocked    dev  \n",
      "699897151  article  blocked  train  \n",
      "\n",
      "[115864 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SktBW4ON_KoZ",
    "outputId": "b9e4c13a-bd45-4282-e46d-b0b81dfa32a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rev_id  worker_id  quoting_attack  recipient_attack  \\\n",
      "0            37675       1362             0.0               0.0   \n",
      "1            37675       2408             0.0               0.0   \n",
      "2            37675       1493             0.0               0.0   \n",
      "3            37675       1439             0.0               0.0   \n",
      "4            37675        170             0.0               0.0   \n",
      "...            ...        ...             ...               ...   \n",
      "1365212  699897151        628             0.0               0.0   \n",
      "1365213  699897151         15             0.0               0.0   \n",
      "1365214  699897151         57             0.0               0.0   \n",
      "1365215  699897151       1815             0.0               0.0   \n",
      "1365216  699897151        472             0.0               0.0   \n",
      "\n",
      "         third_party_attack  other_attack  attack  \n",
      "0                       0.0           0.0     0.0  \n",
      "1                       0.0           0.0     0.0  \n",
      "2                       0.0           0.0     0.0  \n",
      "3                       0.0           0.0     0.0  \n",
      "4                       0.0           0.0     0.0  \n",
      "...                     ...           ...     ...  \n",
      "1365212                 0.0           0.0     0.0  \n",
      "1365213                 0.0           0.0     0.0  \n",
      "1365214                 0.0           0.0     0.0  \n",
      "1365215                 0.0           0.0     0.0  \n",
      "1365216                 0.0           0.0     0.0  \n",
      "\n",
      "[1365217 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL-hpcJk_Kod"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "We've now downloaded the data.  Please open it up and take a look.  How are the data formatted?  What's in there?  What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzHMjOuw_Kod"
   },
   "source": [
    "### Your answer to Question 2\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qouW0MQm_Koe",
    "outputId": "3b6cebb5-953d-4102-ab95-919ba773933f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmNNZ7wI_Koj"
   },
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-ScHsFe_Kon"
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3--YfdY_Kor"
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ok2C9HSh_Ko1",
    "outputId": "ae391540-b7ff-4bf7-f52b-f931ad32979e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279             Iraq is not good  ===  ===  USA is bad   \n",
       "2702703      ____ fuck off you little asshole. If you wan...\n",
       "4632658         i have a dick, its bigger than yours! hahaha\n",
       "6545332      == renault ==  you sad little bpy for drivin...\n",
       "6545351      == renault ==  you sad little bo for driving...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dhdl4X3s_Ko5",
    "outputId": "44658d11-cde8-409f-8a5e-b3c4a0a832a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "699645524     Brandon Semenuk has won the event four times ...\n",
       "699659494    im soory since when is google images not allow...\n",
       "699660419    what ever you fuggin fag Question how did you ...\n",
       "699661020      == Nice try but no cigar........idiot ==  Th...\n",
       "699664687     shut up mind your own business and go fuck so...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LlCsiIq_Ko8",
    "outputId": "72f2202a-1fa7-4819-910f-f985b389bd0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.957\n"
     ]
    }
   ],
   "source": [
    "# fit a simple text classifier\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EeF8uRx_Ko_"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "What has happened here?  Can you explain, in general terms, what this code is doing?  What does ROC AUC mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-rAM8ZZ_Ko_"
   },
   "source": [
    "### Your answer to Question 3\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MFrXwDe_KpA",
    "outputId": "da4fc82d-c390-446c-9876-ac5c2ce5e425"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now try to classify new comments\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXt1m_5f_KpC",
    "outputId": "f309c6e3-a540-47a2-8dc1-69ef8e6060f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeeNTBHm_KpF"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Edit the code above to try out some new nice and nasty comments of your own invention.  Can you \"break\" the classifier?  How/why or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsGsbMrP_KpG"
   },
   "source": [
    "### Your answer to Question 4\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9GVULol_KpH"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Please summarize what has happened in this notebook as if you are explaining it to someone who has never heard of document classification or machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lofLpnsD_KpH"
   },
   "source": [
    "### Your answer to Question 5\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWd3hcpI_KpI"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Now please take a look at the authors' original paper ( https://arxiv.org/abs/1610.08914).  What did they do with these Wikipedia comments?  What was their larger goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqkDIrAs_KpI"
   },
   "source": [
    "### Your answer to Question 6\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8I5cTUw_KpJ"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Please read the Document Classification chapter of our in-progress \"textbook\" and use bullet points to indicate 5 things you learned and/or constructive suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 7\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hw4_wikipedia_personal_attacks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
