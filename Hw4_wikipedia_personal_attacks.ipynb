{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcXO5vB5_KoE"
   },
   "source": [
    "## Wikipedia Personal Attacks\n",
    "\n",
    "The data and most of the code in this Notebook are taken from Ellery Wulczyn, Nithum Thain, and Lucas Dixon. (paper here: https://arxiv.org/abs/1610.08914, notebook here: https://github.com/ewulczyn/wiki-detox/blob/master/src/figshare/Wikipedia%20Talk%20Data%20-%20Getting%20Started.ipynb)\n",
    "\n",
    "These authors' data contain:\n",
    "\n",
    "- a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "- a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "- a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "\n",
    "Please note that some of these comments contain offensive language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdb0h5SL_KoF"
   },
   "source": [
    "## Building a classifier for personal attacks (code from Wulczyn et al)\n",
    "\n",
    "First we import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zRaKhoy_KoF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LtbFQoh_KoM"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "What are these packages and why are we using them?  (Feel free to Google around.)  It is okay if you do not understand all of this, just do your best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xQdX9QX_KoN"
   },
   "source": [
    "### Your answer to Question 1\n",
    "\n",
    "#### Pandas\n",
    "- Pandas is a package for data manipulation and analysis. It makes csv file reading and writing easier.\n",
    "#### urllib \n",
    "- Urllib is a package for url opening and reading, parsing and throwing errors. \n",
    "#### sklearn.pipeline \n",
    "- This package is a Pipeline object of transformers with a final estimator. It's helpful when performing sequences of different transformations and assembling several steps that can be cross-validated together with different parameters.\n",
    "#### CountVectorizer\n",
    "- CountVectorizer is used to convert a collection of text documents to a vector of term or token counts.\n",
    "####  Tfidftransformer\n",
    "- Scikit-learnâ€™s Tfidftransformer converts a collection of raw documents to a matrix of TF-IDF features.  \n",
    "#### LogisticRegression\n",
    "- This is a logistic regression classifier. The output can take only discrete values for given set of inputs.\n",
    "#### roc_auc_score\n",
    "- It \"Compute(s) Area Under the Receiver Operating Characteristic Curve\" (ROC AUC) from prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7O4GCdS_KoN"
   },
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQrNoRDj_KoR"
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M43hpsL6_KoV",
    "outputId": "8fd123ba-b2a0-4aa3-b326-1e213b20872d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     comment  year  logged_in  \\\n",
      "rev_id                                                                          \n",
      "37675      `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
      "44816      `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002      False   \n",
      "49851      NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002      False   \n",
      "89320       Next, maybe you could work on being less cond...  2002       True   \n",
      "93890                   This page will need disambiguation.   2002       True   \n",
      "...                                                      ...   ...        ...   \n",
      "699848324  `NEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENThese ...  2016       True   \n",
      "699851288  NEWLINE_TOKENNEWLINE_TOKENThe Institute for Hi...  2016       True   \n",
      "699857133  NEWLINE_TOKEN:The way you're trying to describ...  2016       True   \n",
      "699891012  NEWLINE_TOKENNEWLINE_TOKEN== Warning ==NEWLINE...  2016       True   \n",
      "699897151  Alternate option===NEWLINE_TOKENIs there perha...  2016       True   \n",
      "\n",
      "                ns   sample  split  \n",
      "rev_id                              \n",
      "37675      article   random  train  \n",
      "44816      article   random  train  \n",
      "49851      article   random  train  \n",
      "89320      article   random    dev  \n",
      "93890      article   random  train  \n",
      "...            ...      ...    ...  \n",
      "699848324  article  blocked  train  \n",
      "699851288  article  blocked   test  \n",
      "699857133  article  blocked  train  \n",
      "699891012     user  blocked    dev  \n",
      "699897151  article  blocked  train  \n",
      "\n",
      "[115864 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SktBW4ON_KoZ",
    "outputId": "b9e4c13a-bd45-4282-e46d-b0b81dfa32a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rev_id  worker_id  quoting_attack  recipient_attack  \\\n",
      "0            37675       1362             0.0               0.0   \n",
      "1            37675       2408             0.0               0.0   \n",
      "2            37675       1493             0.0               0.0   \n",
      "3            37675       1439             0.0               0.0   \n",
      "4            37675        170             0.0               0.0   \n",
      "...            ...        ...             ...               ...   \n",
      "1365212  699897151        628             0.0               0.0   \n",
      "1365213  699897151         15             0.0               0.0   \n",
      "1365214  699897151         57             0.0               0.0   \n",
      "1365215  699897151       1815             0.0               0.0   \n",
      "1365216  699897151        472             0.0               0.0   \n",
      "\n",
      "         third_party_attack  other_attack  attack  \n",
      "0                       0.0           0.0     0.0  \n",
      "1                       0.0           0.0     0.0  \n",
      "2                       0.0           0.0     0.0  \n",
      "3                       0.0           0.0     0.0  \n",
      "4                       0.0           0.0     0.0  \n",
      "...                     ...           ...     ...  \n",
      "1365212                 0.0           0.0     0.0  \n",
      "1365213                 0.0           0.0     0.0  \n",
      "1365214                 0.0           0.0     0.0  \n",
      "1365215                 0.0           0.0     0.0  \n",
      "1365216                 0.0           0.0     0.0  \n",
      "\n",
      "[1365217 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL-hpcJk_Kod"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "We've now downloaded the data.  Please open it up and take a look.  How are the data formatted?  What's in there?  What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzHMjOuw_Kod"
   },
   "source": [
    "### Your answer to Question 2\n",
    "\n",
    "- The data are formatted as a TSV file. Each column is separated from the adjacent one with a tab. \n",
    "- It contains a lot of wikipeadia revision comments and some information including if it was made by a logged in user, if it's for article or user, train data or dev data. The second dataset contains if annotators think the comments are attacks for each entry. \n",
    "- I noticed each comment starts with \"NEWLINE_TOKENNEWLINE\". Need to clean the data by removing those phrases. Also very few entries are labelled as attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qouW0MQm_Koe",
    "outputId": "3b6cebb5-953d-4102-ab95-919ba773933f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmNNZ7wI_Koj"
   },
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-ScHsFe_Kon"
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3--YfdY_Kor"
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ok2C9HSh_Ko1",
    "outputId": "ae391540-b7ff-4bf7-f52b-f931ad32979e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279             Iraq is not good  ===  ===  USA is bad   \n",
       "2702703      ____ fuck off you little asshole. If you wan...\n",
       "4632658         i have a dick, its bigger than yours! hahaha\n",
       "6545332      == renault ==  you sad little bpy for drivin...\n",
       "6545351      == renault ==  you sad little bo for driving...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dhdl4X3s_Ko5",
    "outputId": "44658d11-cde8-409f-8a5e-b3c4a0a832a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "699645524     Brandon Semenuk has won the event four times ...\n",
       "699659494    im soory since when is google images not allow...\n",
       "699660419    what ever you fuggin fag Question how did you ...\n",
       "699661020      == Nice try but no cigar........idiot ==  Th...\n",
       "699664687     shut up mind your own business and go fuck so...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LlCsiIq_Ko8",
    "outputId": "72f2202a-1fa7-4819-910f-f985b389bd0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.957\n"
     ]
    }
   ],
   "source": [
    "# fit a simple text classifier\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EeF8uRx_Ko_"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "What has happened here?  Can you explain, in general terms, what this code is doing?  What does ROC AUC mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-rAM8ZZ_Ko_"
   },
   "source": [
    "### Your answer to Question 3\n",
    "\n",
    "- The code first labels the comments as attack when more than half of the annotators thinks so. Then it add the label to the comments. For data cleaning, it uses a lambda expression to remove the \"NEWLINE_TOKEN\" and other irrelevant phrases for each comment. \n",
    "- Then we print out some entries that are labelled as attack. \"head()\" \".tail()\" are used to show the first few results and last few results.\n",
    "- Lastly we use a simple text classifier to predict how likely the sample comments are \"attacks\"\n",
    "- In general, the code is labelling the comments as \"attack\" or not. It prints some examples of comments labelled as \"attack\" and evaluates how likely the test data are \"attack\".\n",
    "- ROC AUC means Compute Area Under the Receiver Operating Characteristic Curve. Using ROC AUC we could evaluate a logistic regression model many times with different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MFrXwDe_KpA",
    "outputId": "da4fc82d-c390-446c-9876-ac5c2ce5e425"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now try to classify new comments\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXt1m_5f_KpC",
    "outputId": "f309c6e3-a540-47a2-8dc1-69ef8e6060f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Your revision is so brillant that it adds nothing to the article.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Go home kid. No mess with this job'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['You f**** C******* a*****e return to your country!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Go home you Chinese. You should not edit wiki'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['You are such an idiot.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Woman should go home and cook, not editing wiki.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeeNTBHm_KpF"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Edit the code above to try out some new nice and nasty comments of your own invention.  Can you \"break\" the classifier?  How/why or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsGsbMrP_KpG"
   },
   "source": [
    "### Your answer to Question 4\n",
    "\n",
    "- I added a few nasty examples above with sacarstic strategy and racial/ nationality bias. It seems this classifier doesn't label \"You should not edit wiki\" and those negation as personal attack. It only tells by negative word usages such as \"stupid\" or \"idiot\". And it can't tell words with gender/ racial bias. I'm supposing because it's based on TF-IDF method for each word, it doens't understand the negative meaning of phrases without the usage of curse words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9GVULol_KpH"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Please summarize what has happened in this notebook as if you are explaining it to someone who has never heard of document classification or machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lofLpnsD_KpH"
   },
   "source": [
    "### Your answer to Question 5\n",
    "\n",
    "This model helps to classify personal attacks in wikipeadia revision comments. We first train the model on labelled data. Those comments are already labelled by people. We are teaching our model what is personal attack, what is not. Then we can ask the model to predict it for the new data. It's like teaching a kid what a cat is by showing them some animals, and tell them \"this is not a cat\" or \"this is a cat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWd3hcpI_KpI"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Now please take a look at the authors' original paper ( https://arxiv.org/abs/1610.08914).  What did they do with these Wikipedia comments?  What was their larger goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqkDIrAs_KpI"
   },
   "source": [
    "### Your answer to Question 6\n",
    "\n",
    "- After running the classifer, the researchers analyzed the data. They concluded anonymous users are more likely to personal attack the users. And wikipeadia users with higher activity levels are less likely to make personal attack. Personal attacks are also concentrated among a few toxic users.\n",
    "- Their larger goal is to create effective policies to identify and appropriately respond to harassement. The goal is to create a quantitative analysis method using personal attacks wikipeadia data as an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8I5cTUw_KpJ"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Please read the Document Classification chapter of our in-progress \"textbook\" and use bullet points to indicate 5 things you learned and/or constructive suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 7\n",
    "\n",
    "#### What I learned \n",
    "- The perceptron is a network with input and output layer only.It calculates the total score of the edges between nodes and decides which category is more likely.\n",
    "- Pre-trained word embeddings carrys information of words meanings and co-occurence patterns.\n",
    "- Sentiment analysis is also a classifier. We can feed positive/ negative reviewes and train it to tell which is positive review for test data.\n",
    "\n",
    "#### Suggestions \n",
    "- It would be easier to read if the section for BERT (p. 153) has a subtitle. Instead of blending in the discussion of neutral networks. \n",
    "- Would be nice to make a graph to show what's a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hw4_wikipedia_personal_attacks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
